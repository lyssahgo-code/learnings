# Journal — 9-27-2025 — DAY 3

## 1) What I learned (bullets, not prose)
- Dimensional modeling and normalization are *not* an 'either/or' option
- Normalization and star schema may not be required but it is **good practice**, especially to unknown datasets. Cause it makes querying easier
- **RAW**: dump data
- **CLEAN**: where normalized/clean data goes
- **MART**: where star schema data goes, meant for end users
- Testing data before cleaning is important such as comparing the raw vs clean data, if the rows/columns match up, etc (**sense check**:check rows, sums, outliers at each stage)
- Dimensional tables, keep qualitative as much as possible while facts table would hold most your quantitative data
- While technically queries *can* be ran to create/push tables in various tools (Dbeaver, Metabase), it would still be ideal to initiate these processes with git where project repo is for version control and documentation (think, safety net)

## 2) New vocabulary (define in your own words)
- Normalization - breaking data into smaller tables so there’s no duplicate info
- Dimensional Modeling - arranging data into facts (measured) and dimensions (details) so queries are easier
- Sense check - additional validation step to see if the outputs look reasonable so far (ie. data in raw vs clean matches, etc)

## 3) Data Engineering mindset applied (what principles did I use?)
- Do sense/sanity checks at *every stage* (raw > staging > clean > marts)

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- Since there are unexpected outputs in our data, as compared to other groups, we decided to restart the pipeline from ingestion all the way through dbt and modeling. It took more time, but this way we felt sure the fact and dimension tables were based on clean, consistent data

## 5) Open questions (things I still don’t get)
- When is it better to keep some duplicates instead of normalizing everything?
- In dbt, how do I decide when to materialize a model as a table vs. a view?

## 6) Next actions (small, doable steps)
- [ ] Study Git functions and documentation
- [ ] Review best practices for pipeline documentation (DLT, dbt, modeling, marts/visualization)
- [ ] Deep dive on normalization vs. dimensional modeling, and when to use each

## 7) Artifacts & links (code, queries, dashboards)
- Group 1 Git Doc: https://github.com/cardibhie/ftw_de_etl_girls/blob/main/clickhouse/2025-09-20-assignment.md
- Group 1 Metabase Dashboard: https://ftw.dataengineering.ph/public/dashboard/b33664b4-451a-4cff-abd5-4068f66aaf4e

---

### Mini reflection (3–5 sentences)
Restarting the pipeline took us extra time, but it actually helped us trust the results more since our first outputs didn’t match. I learned how even small checks (like row counts between raw and clean) can save us later, but I still feel like I’m not doing enough testing yet. And to be honest, while I get the idea of normalization and dimensional modeling, I’m still unsure about the right moment to switch from cleaning/normalizing to building fact and dimension tables. That’s something I want to practice more in the next projects.
